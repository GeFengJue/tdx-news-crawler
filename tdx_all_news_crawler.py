import requests
import json
import sqlite3
import re
from datetime import datetime, timedelta
import time
from flask import Flask, jsonify, request
from flask_cors import CORS

class TDXAllNewsCrawler:
    def __init__(self):
        self.base_url = "http://fast1.tdx.com.cn:7615"
        self.session = requests.Session()
        
        # è®¾ç½®ç²¾ç¡®çš„è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0',
            'Accept': '*/*',
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Origin': 'http://fast1.tdx.com.cn:7615',
            'Referer': 'http://fast1.tdx.com.cn:7615/site/tdx_zxts/page_main.html?tabsel=0',
            'X-Requested-With': 'XMLHttpRequest',
            'Connection': 'keep-alive',
        }
    
    def init_session(self):
        """åˆå§‹åŒ–ä¼šè¯"""
        try:
            print("ğŸ”„ åˆå§‹åŒ–ä¼šè¯...")
            main_url = f"{self.base_url}/site/tdx_zxts/page_main.html?tabsel=0"
            response = self.session.get(main_url, headers=self.headers, timeout=10)
            print(f"âœ… ä¼šè¯åˆå§‹åŒ–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            return True
        except Exception as e:
            print(f"âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def fetch_page_data(self, page=1, page_size=50):
        """è·å–æŒ‡å®šé¡µé¢çš„æ•°æ®"""
        try:
            # ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºæŸ¥è¯¢æ¡ä»¶ï¼ˆåŸºäºæ‚¨æä¾›çš„æˆåŠŸæ ¼å¼ï¼‰
            current_date = datetime.now().strftime("%Y-%m-%d 00:00:00")
            
            api_url = f"{self.base_url}/TQLEX?Entry=CWServ.tdxzb_zxts_ywbb"
            
            # æ„å»ºè´Ÿè½½æ•°æ®ï¼ˆä½¿ç”¨æ‚¨æä¾›çš„æˆåŠŸå‚æ•°æ ¼å¼ï¼‰
            payload_data = {
                "CallName": "tdxzb_zxts_ywbb",
                "Params": [
                    current_date,  # å½“å‰æ—¥æœŸ
                    "",           # ç©ºå­—ç¬¦ä¸²
                    page,         # é¡µç 
                    page_size,    # æ¯é¡µæ•°é‡
                    "0"           # å…¶ä»–å‚æ•°
                ],
                "secuparse": True,
                "parsefld": "summary",
                "tdxPageID": "_UrlEncode"
            }
            
            json_payload = json.dumps(payload_data, ensure_ascii=False)
            
            print(f"ğŸ“„ è·å–ç¬¬{page}é¡µæ•°æ®...")
            
            response = self.session.post(
                api_url,
                data=json_payload,
                headers=self.headers,
                timeout=15
            )
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    if data.get('ErrorCode') == 0:
                        result_sets = data.get('ResultSets', [])
                        if result_sets:
                            content = result_sets[0].get('Content', [])
                            print(f"âœ… ç¬¬{page}é¡µè·å–åˆ° {len(content)} æ¡æ•°æ®")
                            return data
                        else:
                            print(f"âš ï¸ ç¬¬{page}é¡µæ— æ•°æ®")
                    else:
                        print(f"âŒ ç¬¬{page}é¡µAPIé”™è¯¯: {data.get('ErrorInfo')}")
                except json.JSONDecodeError:
                    print(f"âŒ ç¬¬{page}é¡µJSONè§£æå¤±è´¥")
            else:
                print(f"âŒ ç¬¬{page}é¡µHTTPé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥: {e}")
        
        return None
    
    def fetch_all_news(self, max_pages=10, page_size=50):
        """è·å–æ‰€æœ‰æ–°é—»æ•°æ®"""
        print("ğŸ”„ å¼€å§‹è·å–æ‰€æœ‰æ–°é—»æ•°æ®...")
        
        all_data = []
        total_records = 0
        
        for page in range(1, max_pages + 1):
            page_data = self.fetch_page_data(page, page_size)
            if page_data:
                result_sets = page_data.get('ResultSets', [])
                if result_sets:
                    content = result_sets[0].get('Content', [])
                    if len(content) > 0:
                        all_data.append(page_data)
                        total_records += len(content)
                        print(f"ğŸ“Š ç´¯è®¡è·å–: {total_records} æ¡è®°å½•")
                        
                        # å¦‚æœå½“å‰é¡µæ•°æ®ä¸è¶³ä¸€é¡µï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šæ•°æ®äº†
                        if len(content) < page_size:
                            print("ğŸ“„ å·²è·å–æ‰€æœ‰å¯ç”¨æ•°æ®")
                            break
                    else:
                        print("ğŸ“„ æ— æ›´å¤šæ•°æ®ï¼Œåœæ­¢è·å–")
                        break
                else:
                    print("ğŸ“„ æ— ç»“æœé›†ï¼Œåœæ­¢è·å–")
                    break
            else:
                print("âŒ è·å–æ•°æ®å¤±è´¥ï¼Œåœæ­¢è·å–")
                break
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        print(f"ğŸ‰ è·å–å®Œæˆï¼Œå…± {len(all_data)} é¡µï¼Œ{total_records} æ¡è®°å½•")
        return all_data
    
    def create_database(self, db_name='tdx_all_news.db'):
        """åˆ›å»ºæ•°æ®åº“"""
        conn = sqlite3.connect(db_name)
        cursor = conn.cursor()
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS all_stock_news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            position INTEGER,
            record_id INTEGER UNIQUE,
            title TEXT,
            issue_date TEXT,
            summary TEXT,
            source TEXT,
            relate_id INTEGER,
            proc_id INTEGER,
            mark_id INTEGER,
            stock_code TEXT,
            stock_name TEXT,
            crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_record_id ON all_stock_news(record_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_stock_code ON all_stock_news(stock_code)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_issue_date ON all_stock_news(issue_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON all_stock_news(source)')
        
        conn.commit()
        return conn
    
    def extract_stock_info(self, title):
        """ä»æ ‡é¢˜ä¸­æå–è‚¡ç¥¨ä»£ç å’Œåç§°"""
        pattern = r'([^\(]+)\((\d{6})\)'
        match = re.search(pattern, title)
        if match:
            stock_name = match.group(1).strip()
            stock_code = match.group(2)
            return stock_code, stock_name
        return None, None
    
    def save_all_data(self, conn, all_data):
        """ä¿å­˜æ‰€æœ‰æ•°æ®åˆ°æ•°æ®åº“"""
        if not all_data:
            print("âŒ æ— æ•°æ®å¯ä¿å­˜")
            return 0
        
        cursor = conn.cursor()
        total_inserted = 0
        
        for page_data in all_data:
            if 'ResultSets' not in page_data or len(page_data['ResultSets']) == 0:
                continue
                
            result_set = page_data['ResultSets'][0]
            col_names = result_set['ColName']
            content_data = result_set['Content']
            
            page_inserted = 0
            for row in content_data:
                if len(row) >= len(col_names):
                    data_dict = dict(zip(col_names, row))
                    
                    # æå–è‚¡ç¥¨ä¿¡æ¯
                    title = data_dict.get('title', '')
                    stock_code, stock_name = self.extract_stock_info(title)
                    
                    try:
                        cursor.execute('''
                        INSERT OR IGNORE INTO all_stock_news 
                        (position, record_id, title, issue_date, summary, source, relate_id, proc_id, mark_id, stock_code, stock_name)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            data_dict.get('pos', 0),
                            data_dict.get('rec_id', 0),
                            title,
                            data_dict.get('issue_date', ''),
                            data_dict.get('summary', ''),
                            data_dict.get('src_info', ''),
                            data_dict.get('relate_id', 0),
                            data_dict.get('Proc_Id', 0),
                            data_dict.get('Mark_Id', 0),
                            stock_code,
                            stock_name
                        ))
                        
                        if cursor.rowcount > 0:
                            page_inserted += 1
                            total_inserted += 1
                        
                    except Exception as e:
                        print(f"âŒ æ’å…¥æ•°æ®å¤±è´¥: {e}")
            
            print(f"ğŸ’¾ ç¬¬{all_data.index(page_data) + 1}é¡µä¿å­˜: {page_inserted} æ¡è®°å½•")
        
        conn.commit()
        return total_inserted
    
    def display_top_news(self, conn, limit=50):
        """æ˜¾ç¤ºå‰Næ¡æ–°é—»"""
        cursor = conn.cursor()
        
        print(f"\nğŸ”¥ å‰{limit}æ¡æœ€æ–°å…¬å‘Š:")
        print("=" * 80)
        
        cursor.execute('''
        SELECT title, stock_code, stock_name, issue_date, source, summary
        FROM all_stock_news 
        ORDER BY issue_date DESC 
        LIMIT ?
        ''', (limit,))
        
        news_list = cursor.fetchall()
        
        for i, (title, code, name, date, source, summary) in enumerate(news_list, 1):
            print(f"{i:2d}. {title}")
            if code:
                print(f"    è‚¡ç¥¨: {code} {name}")
            else:
                print(f"    æ–°é—»ç±»å‹")
            print(f"    æ—¶é—´: {date} | æ¥æº: {source}")
            if summary and len(summary) > 100:
                short_summary = summary[:100] + "..."
                print(f"    æ‘˜è¦: {short_summary}")
            print("-" * 80)
        
        return news_list
    
    def show_statistics(self, conn):
        """æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        cursor = conn.cursor()
        
        # æ€»è®°å½•æ•°
        cursor.execute("SELECT COUNT(*) FROM all_stock_news")
        total_count = cursor.fetchone()[0]
        
        # è‚¡ç¥¨ç»Ÿè®¡
        cursor.execute("SELECT COUNT(DISTINCT stock_code) FROM all_stock_news WHERE stock_code IS NOT NULL")
        stock_count = cursor.fetchone()[0]
        
        # æ¥æºç»Ÿè®¡
        cursor.execute("SELECT source, COUNT(*) FROM all_stock_news GROUP BY source ORDER BY COUNT(*) DESC")
        sources = cursor.fetchall()
        
        # æ—¥æœŸèŒƒå›´
        cursor.execute("SELECT MIN(issue_date), MAX(issue_date) FROM all_stock_news")
        min_date, max_date = cursor.fetchone()
        
        print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {total_count}")
        print(f"   è‚¡ç¥¨æ•°é‡: {stock_count}")
        print(f"   æ—¶é—´èŒƒå›´: {min_date} åˆ° {max_date}")
        
        print(f"\nğŸ“° æ¥æºåˆ†å¸ƒ:")
        for source, count in sources:
            percentage = (count / total_count) * 100
            print(f"   {source}: {count} æ¡ ({percentage:.1f}%)")
        
        # çƒ­é—¨è‚¡ç¥¨
        cursor.execute('''
        SELECT stock_code, stock_name, COUNT(*) as count 
        FROM all_stock_news 
        WHERE stock_code IS NOT NULL 
        GROUP BY stock_code, stock_name 
        ORDER BY count DESC 
        LIMIT 10
        ''')
        
        print(f"\nğŸ† çƒ­é—¨è‚¡ç¥¨å…¬å‘Šæ’è¡Œ:")
        for code, name, count in cursor.fetchall():
            print(f"   {code} {name}: {count} æ¡å…¬å‘Š")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("ğŸš€ å¯åŠ¨åŒèŠ±é¡ºå…¨é‡æ–°é—»çˆ¬è™«...")
        print(f"â° å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åˆå§‹åŒ–ä¼šè¯
        if not self.init_session():
            return
        
        # åˆ›å»ºæ•°æ®åº“
        conn = self.create_database('tdx_all_news.db')
        print("âœ… æ•°æ®åº“å‡†å¤‡å°±ç»ª")
        
        # è·å–æ‰€æœ‰æ–°é—»æ•°æ®
        all_data = self.fetch_all_news(max_pages=5, page_size=50)  # è·å–5é¡µï¼Œæ¯é¡µ50æ¡
        
        if all_data:
            # ä¿å­˜æ•°æ®
            saved_count = self.save_all_data(conn, all_data)
            print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡æ•°æ®åˆ°æ•°æ®åº“")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.show_statistics(conn)
            
            # æ˜¾ç¤ºå‰50æ¡æ–°é—»
            self.display_top_news(conn, 50)
            
            # å¯¼å‡ºä¸ºCSV
            self.export_to_csv(conn)
        else:
            print("âŒ æœªèƒ½è·å–åˆ°æ•°æ®")
        
        conn.close()
        print("\nğŸ‰ å…¨é‡æ–°é—»çˆ¬å–å®Œæˆ!")
    
    def export_to_csv(self, conn):
        """å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""
        import csv
        
        cursor = conn.cursor()
        cursor.execute('''
        SELECT position, record_id, title, issue_date, summary, source, 
               stock_code, stock_name, relate_id, proc_id, mark_id
        FROM all_stock_news 
        ORDER BY issue_date DESC
        ''')
        
        with open('tdx_all_news_export.csv', 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow(['åºå·', 'è®°å½•ID', 'æ ‡é¢˜', 'å‘å¸ƒæ—¶é—´', 'æ‘˜è¦', 'æ¥æº', 
                           'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'å…³è”ID', 'å¤„ç†ID', 'æ ‡è®°ID'])
            
            # å†™å…¥æ•°æ®
            for row in cursor.fetchall():
                writer.writerow(row)
        
        print(f"ğŸ“„ æ•°æ®å·²å¯¼å‡ºåˆ°: tdx_all_news_export.csv")

class TDXNewsAPI:
    def __init__(self, db_name='tdx_all_news.db'):
        self.db_name = db_name
        self.app = Flask(__name__)
        CORS(self.app)  # å¯ç”¨CORSæ”¯æŒ
        self.setup_routes()
    
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(self.db_name)
        conn.row_factory = sqlite3.Row
        return conn
    
    def setup_routes(self):
        """è®¾ç½®APIè·¯ç”±"""
        
        @self.app.route('/api/news', methods=['GET'])
        def get_all_news():
            """è·å–æ‰€æœ‰æ–°é—»"""
            page = request.args.get('page', 1, type=int)
            limit = request.args.get('limit', 50, type=int)
            offset = (page - 1) * limit
            
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM all_stock_news')
            total = cursor.fetchone()[0]
            
            cursor.execute('''
            SELECT * FROM all_stock_news 
            ORDER BY issue_date DESC 
            LIMIT ? OFFSET ?
            ''', (limit, offset))
            
            news_list = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            return jsonify({
                'success': True,
                'total': total,
                'page': page,
                'limit': limit,
                'data': news_list
            })
        
        @self.app.route('/api/news/<int:news_id>', methods=['GET'])
        def get_news_by_id(news_id):
            """æ ¹æ®IDè·å–æ–°é—»è¯¦æƒ…"""
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute('SELECT * FROM all_stock_news WHERE id = ?', (news_id,))
            news = cursor.fetchone()
            conn.close()
            
            if news:
                return jsonify({
                    'success': True,
                    'data': dict(news)
                })
            else:
                return jsonify({
                    'success': False,
                    'message': 'æ–°é—»ä¸å­˜åœ¨'
                }), 404
        
        @self.app.route('/api/news/search', methods=['GET'])
        def search_news():
            """æœç´¢æ–°é—»"""
            keyword = request.args.get('q', '')
            page = request.args.get('page', 1, type=int)
            limit = request.args.get('limit', 50, type=int)
            offset = (page - 1) * limit
            
            if not keyword:
                return jsonify({
                    'success': False,
                    'message': 'è¯·è¾“å…¥æœç´¢å…³é”®è¯'
                }), 400
            
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute('''
            SELECT COUNT(*) FROM all_stock_news 
            WHERE title LIKE ? OR summary LIKE ?
            ''', (f'%{keyword}%', f'%{keyword}%'))
            total = cursor.fetchone()[0]
            
            cursor.execute('''
            SELECT * FROM all_stock_news 
            WHERE title LIKE ? OR summary LIKE ?
            ORDER BY issue_date DESC 
            LIMIT ? OFFSET ?
            ''', (f'%{keyword}%', f'%{keyword}%', limit, offset))
            
            news_list = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            return jsonify({
                'success': True,
                'total': total,
                'page': page,
                'limit': limit,
                'keyword': keyword,
                'data': news_list
            })
        
        @self.app.route('/api/news/stocks', methods=['GET'])
        def get_stock_news():
            """è·å–è‚¡ç¥¨ç›¸å…³æ–°é—»"""
            stock_code = request.args.get('code', '')
            page = request.args.get('page', 1, type=int)
            limit = request.args.get('limit', 50, type=int)
            offset = (page - 1) * limit
            
            if not stock_code:
                return jsonify({
                    'success': False,
                    'message': 'è¯·è¾“å…¥è‚¡ç¥¨ä»£ç '
                }), 400
            
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM all_stock_news WHERE stock_code = ?', (stock_code,))
            total = cursor.fetchone()[0]
            
            cursor.execute('''
            SELECT * FROM all_stock_news 
            WHERE stock_code = ?
            ORDER BY issue_date DESC 
            LIMIT ? OFFSET ?
            ''', (stock_code, limit, offset))
            
            news_list = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            return jsonify({
                'success': True,
                'total': total,
                'page': page,
                'limit': limit,
                'stock_code': stock_code,
                'data': news_list
            })
        
        @self.app.route('/api/news/sources', methods=['GET'])
        def get_sources():
            """è·å–æ–°é—»æ¥æºç»Ÿè®¡"""
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute('''
            SELECT source, COUNT(*) as count 
            FROM all_stock_news 
            GROUP BY source 
            ORDER BY count DESC
            ''')
            
            sources = [{'source': row[0], 'count': row[1]} for row in cursor.fetchall()]
            conn.close()
            
            return jsonify({
                'success': True,
                'data': sources
            })
        
        @self.app.route('/api/news/statistics', methods=['GET'])
        def get_statistics():
            """è·å–ç»Ÿè®¡ä¿¡æ¯"""
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # æ€»è®°å½•æ•°
            cursor.execute("SELECT COUNT(*) FROM all_stock_news")
            total_count = cursor.fetchone()[0]
            
            # è‚¡ç¥¨æ•°é‡
            cursor.execute("SELECT COUNT(DISTINCT stock_code) FROM all_stock_news WHERE stock_code IS NOT NULL")
            stock_count = cursor.fetchone()[0]
            
            # æ—¥æœŸèŒƒå›´
            cursor.execute("SELECT MIN(issue_date), MAX(issue_date) FROM all_stock_news")
            min_date, max_date = cursor.fetchone()
            
            # çƒ­é—¨è‚¡ç¥¨
            cursor.execute('''
            SELECT stock_code, stock_name, COUNT(*) as count 
            FROM all_stock_news 
            WHERE stock_code IS NOT NULL 
            GROUP BY stock_code, stock_name 
            ORDER BY count DESC 
            LIMIT 10
            ''')
            
            hot_stocks = [{'code': row[0], 'name': row[1], 'count': row[2]} for row in cursor.fetchall()]
            
            conn.close()
            
            return jsonify({
                'success': True,
                'data': {
                    'total_count': total_count,
                    'stock_count': stock_count,
                    'date_range': {
                        'min': min_date,
                        'max': max_date
                    },
                    'hot_stocks': hot_stocks
                }
            })
        
        @self.app.route('/api/health', methods=['GET'])
        def health_check():
            """å¥åº·æ£€æŸ¥"""
            return jsonify({
                'success': True,
                'message': 'APIæœåŠ¡è¿è¡Œæ­£å¸¸',
                'timestamp': datetime.now().isoformat()
            })
    
    def run_api(self, host='127.0.0.1', port=5000, debug=False):
        """å¯åŠ¨APIæœåŠ¡"""
        print(f"ğŸš€ å¯åŠ¨æ–°é—»APIæœåŠ¡: http://{host}:{port}")
        print("ğŸ“‹ å¯ç”¨æ¥å£:")
        print("   GET /api/news - è·å–æ‰€æœ‰æ–°é—»")
        print("   GET /api/news/<id> - æ ¹æ®IDè·å–æ–°é—»è¯¦æƒ…")
        print("   GET /api/news/search?q=<keyword> - æœç´¢æ–°é—»")
        print("   GET /api/news/stocks?code=<stock_code> - è·å–è‚¡ç¥¨ç›¸å…³æ–°é—»")
        print("   GET /api/news/sources - è·å–æ–°é—»æ¥æºç»Ÿè®¡")
        print("   GET /api/news/statistics - è·å–ç»Ÿè®¡ä¿¡æ¯")
        print("   GET /api/health - å¥åº·æ£€æŸ¥")
        
        self.app.run(host=host, port=port, debug=debug)

import threading
import schedule
import time

def auto_crawl_job():
    """è‡ªåŠ¨çˆ¬è™«ä»»åŠ¡"""
    print(f"ğŸ”„ [{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹è‡ªåŠ¨çˆ¬å–æ–°é—»æ•°æ®...")
    crawler = TDXAllNewsCrawler()
    
    # åˆå§‹åŒ–ä¼šè¯
    if not crawler.init_session():
        print("âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥")
        return
    
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    conn = crawler.create_database('tdx_all_news.db')
    
    # è·å–æ–°é—»æ•°æ®ï¼ˆåªè·å–æœ€æ–°çš„ä¸€é¡µæ•°æ®ï¼Œé¿å…é‡å¤ï¼‰
    page_data = crawler.fetch_page_data(page=1, page_size=50)
    
    if page_data:
        # ä¿å­˜æ•°æ®
        saved_count = crawler.save_all_data(conn, [page_data])
        print(f"âœ… è‡ªåŠ¨çˆ¬å–å®Œæˆï¼Œæ–°å¢ {saved_count} æ¡æ•°æ®")
        
        # æ˜¾ç¤ºæœ€æ–°ç»Ÿè®¡
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM all_stock_news')
        total_count = cursor.fetchone()[0]
        cursor.execute('SELECT MAX(issue_date) FROM all_stock_news')
        latest_date = cursor.fetchone()[0]
        print(f"ğŸ“Š æ•°æ®åº“æ€»è®¡: {total_count} æ¡è®°å½•ï¼Œæœ€æ–°æ—¶é—´: {latest_date}")
    else:
        print("âŒ è‡ªåŠ¨çˆ¬å–å¤±è´¥")
    
    conn.close()

def run_auto_crawler():
    """è¿è¡Œè‡ªåŠ¨çˆ¬è™«è°ƒåº¦å™¨"""
    print("ğŸ¤– å¯åŠ¨è‡ªåŠ¨çˆ¬è™«è°ƒåº¦å™¨...")
    print("â° çˆ¬è™«å°†æ¯åˆ†é’Ÿè‡ªåŠ¨è¿è¡Œä¸€æ¬¡")
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    auto_crawl_job()
    
    # è®¾ç½®æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    schedule.every(1).minutes.do(auto_crawl_job)
    
    while True:
        schedule.run_pending()
        time.sleep(1)

def main():
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == 'api':
            # å¯åŠ¨APIæœåŠ¡
            api = TDXNewsAPI()
            api.run_api()
        elif sys.argv[1] == 'auto':
            # å¯åŠ¨è‡ªåŠ¨çˆ¬è™«
            run_auto_crawler()
        elif sys.argv[1] == 'crawl':
            # å•æ¬¡çˆ¬è™«è¿è¡Œ
            crawler = TDXAllNewsCrawler()
            crawler.run()
        else:
            print("ç”¨æ³•:")
            print("  python tdx_all_news_crawler.py api     - å¯åŠ¨APIæœåŠ¡")
            print("  python tdx_all_news_crawler.py auto    - å¯åŠ¨è‡ªåŠ¨çˆ¬è™«")
            print("  python tdx_all_news_crawler.py crawl   - å•æ¬¡çˆ¬è™«è¿è¡Œ")
    else:
        # é»˜è®¤è¿è¡Œå•æ¬¡çˆ¬è™«
        crawler = TDXAllNewsCrawler()
        crawler.run()

if __name__ == "__main__":
    main()