#!/usr/bin/env python3
"""
GitHub Actionsä¸“ç”¨çš„æ–°é—»çˆ¬è™«è„šæœ¬
æ¯15åˆ†é’Ÿè‡ªåŠ¨è¿è¡Œï¼Œæ›´æ–°æ–°é—»æ•°æ®åº“
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from tdx_all_news_crawler import TDXAllNewsCrawler
import sqlite3
import pandas as pd
from datetime import datetime

def main():
    print("ğŸš€ GitHub Actionsæ–°é—»çˆ¬è™«å¯åŠ¨")
    print(f"â° å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        crawler = TDXAllNewsCrawler()
        
        # åˆå§‹åŒ–ä¼šè¯
        if not crawler.init_session():
            print("âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥")
            return 1
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        conn = crawler.create_database('tdx_all_news.db')
        
        # è·å–æ–°é—»æ•°æ®ï¼ˆè·å–å‰3é¡µï¼Œæ¯é¡µ50æ¡ï¼‰
        all_data = []
        for page in range(1, 4):
            page_data = crawler.fetch_page_data(page, 50)
            if page_data:
                all_data.append(page_data)
                print(f"âœ… ç¬¬{page}é¡µè·å–æˆåŠŸ")
            else:
                print(f"âŒ ç¬¬{page}é¡µè·å–å¤±è´¥")
                break
        
        if all_data:
            # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
            saved_count = crawler.save_all_data(conn, all_data)
            print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡æ•°æ®åˆ°æ•°æ®åº“")
            
            # å¯¼å‡ºä¸ºCSVæ–‡ä»¶
            export_to_csv(conn)
            
            # å¯¼å‡ºä¸ºJSONæ–‡ä»¶ä¾›ç½‘ç«™ä½¿ç”¨
            export_to_json(conn)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            show_statistics(conn)
        else:
            print("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ•°æ®")
        
        conn.close()
        print("\nğŸ‰ GitHub Actionsçˆ¬è™«ä»»åŠ¡å®Œæˆ!")
        return 0
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
        return 1

def export_to_csv(conn):
    """å¯¼å‡ºæ•°æ®åº“ä¸ºCSVæ–‡ä»¶"""
    try:
        cursor = conn.cursor()
        cursor.execute('''
        SELECT position, record_id, title, issue_date, summary, source, 
               stock_code, stock_name, relate_id, proc_id, mark_id
        FROM all_stock_news 
        ORDER BY issue_date DESC
        ''')
        
        # è·å–åˆ—å
        columns = [description[0] for description in cursor.description]
        data = cursor.fetchall()
        
        # åˆ›å»ºDataFrameå¹¶å¯¼å‡º
        df = pd.DataFrame(data, columns=columns)
        df.to_csv('tdx_all_news_export.csv', index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ æ•°æ®å·²å¯¼å‡ºåˆ°CSVæ–‡ä»¶ï¼Œå…± {len(df)} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âŒ CSVå¯¼å‡ºå¤±è´¥: {e}")

def export_to_json(conn):
    """å¯¼å‡ºæ•°æ®åº“ä¸ºJSONæ–‡ä»¶ä¾›ç½‘ç«™ä½¿ç”¨"""
    try:
        cursor = conn.cursor()
        cursor.execute('''
        SELECT record_id, title, issue_date, summary, source, mark_id
        FROM all_stock_news 
        ORDER BY issue_date DESC
        LIMIT 200
        ''')
        
        data = cursor.fetchall()
        
        # è½¬æ¢ä¸ºç½‘ç«™éœ€è¦çš„æ ¼å¼
        news_list = []
        for record in data:
            news_list.append({
                'id': record[0],
                'title': record[1],
                'date': record[2],
                'time': record[2].split(' ')[1] if record[2] and ' ' in record[2] else '--:--:--',
                'content': record[3] or record[1],
                'source': record[4] or 'æœªçŸ¥æ¥æº',
                'highlight': record[5] == 1
            })
        
        # ç”ŸæˆJSONæ–‡ä»¶
        import json
        json_data = {
            'success': True,
            'data': news_list,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'count': len(news_list)
        }
        
        with open('latest_news.json', 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ æ•°æ®å·²å¯¼å‡ºåˆ°JSONæ–‡ä»¶ï¼Œå…± {len(news_list)} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âŒ JSONå¯¼å‡ºå¤±è´¥: {e}")

def show_statistics(conn):
    """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
    try:
        cursor = conn.cursor()
        
        # æ€»è®°å½•æ•°
        cursor.execute("SELECT COUNT(*) FROM all_stock_news")
        total_count = cursor.fetchone()[0]
        
        # æœ€æ–°è®°å½•æ—¶é—´
        cursor.execute("SELECT MAX(issue_date) FROM all_stock_news")
        latest_date = cursor.fetchone()[0]
        
        # æ¥æºç»Ÿè®¡
        cursor.execute("SELECT source, COUNT(*) FROM all_stock_news GROUP BY source ORDER BY COUNT(*) DESC LIMIT 5")
        top_sources = cursor.fetchall()
        
        print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {total_count}")
        print(f"   æœ€æ–°è®°å½•: {latest_date}")
        print(f"   ä¸»è¦æ¥æº:")
        for source, count in top_sources:
            percentage = (count / total_count) * 100 if total_count > 0 else 0
            print(f"     {source}: {count} æ¡ ({percentage:.1f}%)")
            
    except Exception as e:
        print(f"âŒ ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {e}")

if __name__ == "__main__":
    sys.exit(main())