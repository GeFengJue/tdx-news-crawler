import requests
import json
import sqlite3
from urllib.parse import urlencode
import time
from datetime import datetime

class NewsCrawler:
    def __init__(self, db_path='news_data.db'):
        self.db_path = db_path
        self.api_url = "http://localhost:8000/TQLEX"  # ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿå™¨
        self.headers = {
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
    def setup_database(self):
        """åˆ›å»ºæ•°æ®åº“å’Œè¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæ–°é—»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_announcements (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pos INTEGER,
                rec_id INTEGER UNIQUE,
                title TEXT,
                issue_date TEXT,
                summary TEXT,
                src_info TEXT,
                relate_id INTEGER,
                proc_id INTEGER,
                mark_id INTEGER,
                crawl_time TEXT,
                stock_code TEXT,
                stock_name TEXT
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_rec_id ON news_announcements(rec_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_issue_date ON news_announcements(issue_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_stock_code ON news_announcements(stock_code)')
        
        conn.commit()
        conn.close()
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def extract_stock_info(self, title):
        """ä»æ ‡é¢˜ä¸­æå–è‚¡ç¥¨ä»£ç å’Œåç§°"""
        import re
        
        # åŒ¹é…æ¨¡å¼: è‚¡ç¥¨åç§°(ä»£ç ):å…¬å‘Šæ ‡é¢˜
        pattern = r'(.+?)\((\d{6})\):'
        match = re.search(pattern, title)
        
        if match:
            stock_name = match.group(1).strip()
            stock_code = match.group(2)
            return stock_code, stock_name
        return None, None
    
    def fetch_news_data(self):
        """ä»APIè·å–æ–°é—»æ•°æ®"""
        payload = {"Entry": "CWServ.tdxzb_zxts_ywbb"}
        
        try:
            response = requests.post(
                self.api_url,
                data=urlencode(payload),
                headers=self.headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                if data.get('ErrorCode') == 0 and 'ResultSets' in data:
                    return data['ResultSets'][0]  # è¿”å›ç¬¬ä¸€ä¸ªç»“æœé›†
                else:
                    print(f"APIé”™è¯¯: {data.get('ErrorInfo')}")
            else:
                print(f"HTTPé”™è¯¯: {response.status_code}")
                
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
        
        return None
    
    def save_to_database(self, result_set):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        if not result_set:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        col_names = result_set['ColName']
        content = result_set['Content']
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        success_count = 0
        duplicate_count = 0
        error_count = 0
        
        for item in content:
            try:
                # æ„å»ºæ•°æ®å­—å…¸
                data_dict = dict(zip(col_names, item))
                
                # æå–è‚¡ç¥¨ä¿¡æ¯
                stock_code, stock_name = self.extract_stock_info(data_dict['title'])
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                cursor.execute('SELECT id FROM news_announcements WHERE rec_id = ?', (data_dict['rec_id'],))
                if cursor.fetchone():
                    duplicate_count += 1
                    continue
                
                # æ’å…¥æ•°æ®
                cursor.execute('''
                    INSERT INTO news_announcements 
                    (pos, rec_id, title, issue_date, summary, src_info, relate_id, proc_id, mark_id, crawl_time, stock_code, stock_name)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data_dict['pos'],
                    data_dict['rec_id'],
                    data_dict['title'],
                    data_dict['issue_date'],
                    data_dict['summary'],
                    data_dict['src_info'],
                    data_dict['relate_id'],
                    data_dict['Proc_Id'],
                    data_dict['Mark_Id'],
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    stock_code,
                    stock_name
                ))
                
                success_count += 1
                
            except Exception as e:
                print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
                error_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: æˆåŠŸ {success_count} æ¡, é‡å¤ {duplicate_count} æ¡, é”™è¯¯ {error_count} æ¡")
        return success_count > 0
    
    def get_statistics(self):
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ€»è®°å½•æ•°
        cursor.execute('SELECT COUNT(*) FROM news_announcements')
        total_count = cursor.fetchone()[0]
        
        # æŒ‰æ¥æºç»Ÿè®¡
        cursor.execute('SELECT src_info, COUNT(*) FROM news_announcements GROUP BY src_info')
        src_stats = cursor.fetchall()
        
        # æœ€æ–°å…¬å‘Šæ—¶é—´
        cursor.execute('SELECT MAX(issue_date) FROM news_announcements')
        latest_date = cursor.fetchone()[0]
        
        conn.close()
        
        print(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {total_count}")
        print(f"   æœ€æ–°å…¬å‘Š: {latest_date}")
        print(f"   æ¥æºåˆ†å¸ƒ:")
        for src, count in src_stats:
            print(f"     {src}: {count} æ¡")
    
    def crawl(self):
        """æ‰§è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹"""
        print("ğŸš€ å¼€å§‹çˆ¬å–æ–°é—»æ•°æ®...")
        print(f"ğŸ“¡ APIåœ°å€: {self.api_url}")
        print(f"ğŸ’¾ æ•°æ®åº“: {self.db_path}")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.setup_database()
        
        # è·å–æ•°æ®
        print("\nğŸ“¥ ä»APIè·å–æ•°æ®...")
        result_set = self.fetch_news_data()
        
        if result_set:
            print(f"ğŸ“Š è·å–åˆ° {len(result_set['Content'])} æ¡å…¬å‘Šæ•°æ®")
            
            # ä¿å­˜æ•°æ®
            print("\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
            self.save_to_database(result_set)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
            self.get_statistics()
            
            print("\nğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        else:
            print("âŒ æ•°æ®è·å–å¤±è´¥")

def main():
    """ä¸»å‡½æ•°"""
    crawler = NewsCrawler()
    crawler.crawl()

if __name__ == "__main__":
    main()1