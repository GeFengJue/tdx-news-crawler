import requests
import sqlite3
import json
import time
import re
from datetime import datetime

class TDXCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "http://fast1.tdx.com.cn:7615"
        self.api_url = f"{self.base_url}/TQLEX?Entry=CWServ.tdxzb_zxts_ywbb"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Origin': 'http://fast1.tdx.com.cn:7615',
            'Referer': 'http://fast1.tdx.com.cn:7615/site/tdx_zxts/page_main.html?tabsel=0',
            'X-Requested-With': 'XMLHttpRequest',
            'Connection': 'keep-alive',
        }
        
    def init_session(self):
        """åˆå§‹åŒ–ä¼šè¯ï¼Œè·å–å¿…è¦çš„Cookie"""
        try:
            print("ğŸ”„ åˆå§‹åŒ–ä¼šè¯...")
            # è®¿é—®ä¸»é¡µé¢è·å–Cookie
            main_url = f"{self.base_url}/site/tdx_zxts/page_main.html?tabsel=0"
            response = self.session.get(main_url, headers=self.headers, timeout=10)
            print(f"âœ… ä¼šè¯åˆå§‹åŒ–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            return True
        except Exception as e:
            print(f"âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def fetch_real_time_data(self):
        """è·å–å®æ—¶æ•°æ®"""
        try:
            print("ğŸ”„ æ­£åœ¨è·å–å®æ—¶æ•°æ®...")
            
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            payload = {
                'Entry': 'CWServ.tdxzb_zxts_ywbb'
            }
            
            # å‘é€è¯·æ±‚
            response = self.session.post(
                self.api_url,
                data=payload,
                headers=self.headers,
                timeout=15
            )
            
            print(f"ğŸ“Š å“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    if data.get('ErrorCode') == 0:
                        print("âœ… æˆåŠŸè·å–å®æ—¶æ•°æ®!")
                        return data
                    else:
                        print(f"âš ï¸ APIè¿”å›é”™è¯¯: {data.get('ErrorCode')} - {data.get('ErrorInfo', 'æœªçŸ¥é”™è¯¯')}")
                        return None
                except json.JSONDecodeError:
                    print(f"âŒ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSON: {response.text[:100]}...")
                    return None
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def create_database(self):
        """åˆ›å»ºæ•°æ®åº“"""
        conn = sqlite3.connect('realtime_news.db')
        cursor = conn.cursor()
        
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS stock_news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            position INTEGER,
            record_id INTEGER UNIQUE,
            title TEXT,
            issue_date TEXT,
            summary TEXT,
            source TEXT,
            relate_id INTEGER,
            proc_id INTEGER,
            mark_id INTEGER,
            stock_code TEXT,
            stock_name TEXT,
            crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_record_id ON stock_news(record_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_stock_code ON stock_news(stock_code)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_issue_date ON stock_news(issue_date)')
        
        conn.commit()
        return conn
    
    def extract_stock_info(self, title):
        """ä»æ ‡é¢˜ä¸­æå–è‚¡ç¥¨ä»£ç å’Œåç§°"""
        pattern = r'([^\(]+)\((\d{6})\)'
        match = re.search(pattern, title)
        if match:
            stock_name = match.group(1).strip()
            stock_code = match.group(2)
            return stock_code, stock_name
        return None, None
    
    def save_to_database(self, conn, data):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        if not data or 'ResultSets' not in data or len(data['ResultSets']) == 0:
            print("âŒ æ— æ•ˆçš„æ•°æ®æ ¼å¼")
            return 0
        
        cursor = conn.cursor()
        result_set = data['ResultSets'][0]
        col_names = result_set['ColName']
        content_data = result_set['Content']
        
        inserted_count = 0
        for row in content_data:
            data_dict = dict(zip(col_names, row))
            
            # æå–è‚¡ç¥¨ä¿¡æ¯
            stock_code, stock_name = self.extract_stock_info(data_dict['title'])
            
            try:
                cursor.execute('''
                INSERT OR IGNORE INTO stock_news 
                (position, record_id, title, issue_date, summary, source, relate_id, proc_id, mark_id, stock_code, stock_name)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data_dict['pos'],
                    data_dict['rec_id'],
                    data_dict['title'],
                    data_dict['issue_date'],
                    data_dict['summary'],
                    data_dict['src_info'],
                    data_dict['relate_id'],
                    data_dict['Proc_Id'],
                    data_dict['Mark_Id'],
                    stock_code,
                    stock_name
                ))
                
                inserted_count += cursor.rowcount
                
            except Exception as e:
                print(f"âŒ æ’å…¥æ•°æ®å¤±è´¥: {e}")
        
        conn.commit()
        return inserted_count
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("ğŸš€ å¯åŠ¨åŒèŠ±é¡ºå®æ—¶æ•°æ®çˆ¬è™«...")
        print(f"â° å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åˆå§‹åŒ–ä¼šè¯
        if not self.init_session():
            return
        
        # åˆ›å»ºæ•°æ®åº“
        conn = self.create_database()
        print("âœ… æ•°æ®åº“å‡†å¤‡å°±ç»ª")
        
        # è·å–å®æ—¶æ•°æ®
        real_time_data = self.fetch_real_time_data()
        
        if real_time_data:
            # ä¿å­˜æ•°æ®
            saved_count = self.save_to_database(conn, real_time_data)
            print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡å®æ—¶æ•°æ®åˆ°æ•°æ®åº“")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM stock_news")
            total_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT stock_code) FROM stock_news WHERE stock_code IS NOT NULL")
            stock_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT source) FROM stock_news")
            source_count = cursor.fetchone()[0]
            
            print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
            print(f"   æ€»è®°å½•æ•°: {total_count}")
            print(f"   è‚¡ç¥¨æ•°é‡: {stock_count}")
            print(f"   æ¥æºæ•°é‡: {source_count}")
            
            # æ˜¾ç¤ºæœ€æ–°å‡ æ¡æ•°æ®
            cursor.execute("""
            SELECT title, stock_code, issue_date, source 
            FROM stock_news 
            ORDER BY issue_date DESC 
            LIMIT 5
            """)
            
            print(f"\nğŸ“° æœ€æ–°å…¬å‘Š:")
            for i, (title, code, date, source) in enumerate(cursor.fetchall(), 1):
                print(f"   {i}. {title} ({code}) - {source} - {date}")
        
        # å…³é—­è¿æ¥
        conn.close()
        print("\nğŸ‰ çˆ¬è™«æ‰§è¡Œå®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    crawler = TDXCrawler()
    crawler.run()

if __name__ == "__main__":
    main()